
# FOR API AND LOCAL VERSIONS

metadata:
  version: "1.0"
  author: "ER"
  description: "This notebook evaluates different LLM models across various benchmark types such as MMLU, FinanceQA, and Contextual QA."
  supported_models:
    local: 
      # - llama3.1_az_8q
      # - llama3.1_az
      # - llama3.1_az_v2
      - "llama3.2:3b-instruct-fp16"  
    # api: 
    #   - gpt-4o-mini # uncomment if you want to use api based models
  benchmark_types:
    # QA: "Handles questions with simple Q&A format"
    # ContextQA: "Handles questions with context and answers"
    # Arzuman: "Handles questions with options where one is correct"
    # Reshad: "Handles questions with topic-based options where one is correct"
    ARC: "Handles questions with multiple-choice answers, focusing on reasoning and context to determine the correct option."
  dataset_naming_convention:
    # _qa: "QA"
    # _cqa: "ContextQA"
    # _mmlu_fqa: "Arzuman"
    # _tc: "Reshad"
    _mmlu_arc: "ARC"
  selection:
    mode:
      - local
      # - api # uncomment if you want to use api based models
dataset_files:
  # - "datasets/input_datasets/LLM_BENCH_qa.xlsx"
  # - "datasets/input_datasets/Quad_benchmark_cqa.xlsx"
  # - "datasets/input_datasets/banking-benchmark-405b-vs-8b_latest_mmlu_fqa.xlsx"
  # - "datasets/input_datasets/LLM-Benchmark-reshad_tc.xlsx" # Reshad's old version with eng topic names
  # - "datasets/input_datasets/banking_support_updated_aze_version_reshad_tc.xlsx" # Reshad's updated aze. topic names
  - "datasets/input_datasets/arc_translated_mmlu_arc.xlsx"

output:
  results_file: "datasets/output_datasets/benchmark_results.xlsx"

