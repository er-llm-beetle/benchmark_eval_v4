
# ONLY FOR LOCAL VERSIONS

metadata:
  version: "1.0"
  author: "ER"
  description: "This notebook evaluates different LLM models across various benchmark types such as MMLU, FinanceQA, and Contextual QA."
  supported_models:
    - llama3.1_az_8q
    - llama3.1_az
    - llama3.1_az_v2
    - llama3.2:3b-instruct-fp16
  benchmark_types:
    # QA: "Handles questions with simple Q&A format"
    # ContextQA: "Handles questions with context and answers"
    # Arzuman: "Handles questions with options where one is correct"
    # Reshad: "Handles questions with topic-based options where one is correct"
    ARC: "Handles questions with multiple-choice answers, focusing on reasoning and context to determine the correct option."
  dataset_naming_convention:
    # _qa: "QA"
    # _cqa: "ContextQA"
    # _mmlu_fqa: "Arzuman"
    # _tc: "Reshad"
    # _mmlu_arc: "ARC"

dataset_files:
  # - "datasets/input_datasets/LLM_BENCH_qa.xlsx"
  # - "datasets/input_datasets/Quad_benchmark_cqa.xlsx"
  # - "datasets/input_datasets/banking-benchmark-405b-vs-8b_latest_mmlu_fqa.xlsx"
  # - "datasets/input_datasets/LLM-Benchmark-reshad_tc.xlsx"
  - "datasets/input_datasets/arc_translated_mmlu_arc.xlsx"

output:
  results_file: "datasets/output_datasets/benchmark_results.xlsx"
